{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b78cfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math #get log\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections.abc import Iterable\n",
    "from preprocess import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0354eef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "document1 = Path('1.txt').read_text()\n",
    "document2 = Path('2.txt').read_text()\n",
    "document3 = Path('3.txt').read_text()\n",
    "document4 = Path('4.txt').read_text()\n",
    "document5 = Path('5.txt').read_text()\n",
    "queryfile = Path('Queryy.txt').read_text() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eed3672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained deep convolutional neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(queryfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae8db5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " trained deep convolutional neural network 60 million parameters 650000 neurons consists five convolutional layers followed max pooling layers three fully connected layers final 1000 way softmax classify million high resolution images imagenet lsvrc 2010 contest 1000 different classes\n"
     ]
    }
   ],
   "source": [
    "processed_query = []\n",
    "processed_doc1 = []\n",
    "processed_doc2 = []\n",
    "processed_doc3 = []\n",
    "processed_doc4 = []\n",
    "processed_doc5 = []\n",
    "\n",
    "processed_query.append(str(preprocess(queryfile)))\n",
    "processed_doc1.append(str(preprocess(document1)))\n",
    "processed_doc2.append(str(preprocess(document2)))\n",
    "processed_doc3.append(str(preprocess(document3)))\n",
    "processed_doc4.append(str(preprocess(document4)))\n",
    "processed_doc5.append(str(preprocess(document5)))\n",
    "\n",
    "def listToString(s): \n",
    "    \n",
    "    # initialize an empty string\n",
    "    str1 = \"\" \n",
    "    \n",
    "    # traverse in the string  \n",
    "    for ele in s: \n",
    "        str1 += ele  \n",
    "    \n",
    "    # return string  \n",
    "    return str1 \n",
    "\n",
    " \n",
    "queryString = listToString(processed_query)\n",
    "doc1 = listToString(processed_doc1)\n",
    "doc2 = listToString(processed_doc2)\n",
    "doc3 = listToString(processed_doc3)\n",
    "doc4 = listToString(processed_doc4)\n",
    "doc5 = listToString(processed_doc5)\n",
    "\n",
    "print(queryString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "852b9ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained deep convolutional neural network 60 million parameters 650000 neurons consists five layers followed max pooling three fully connected final 1000 way softmax classify high resolution images imagenet lsvrc 2010 contest different classes\n"
     ]
    }
   ],
   "source": [
    "#remove duplicates from query file - create a unique list\n",
    "def unique_list(text_str):\n",
    "    l = text_str.split()\n",
    "    temp = []\n",
    "    for x in l:\n",
    "        if x not in temp:\n",
    "            temp.append(x)\n",
    "    return ' '.join(temp)\n",
    "query = unique_list(queryString)\n",
    "\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbd98040",
   "metadata": {},
   "outputs": [],
   "source": [
    "#end of preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25a592ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Document     price  mse  direction  applied  tailored  future  \\\n",
      "0  Term Frequency  1      5    1          1        1         1       2   \n",
      "\n",
      "   containing  stocks  ...  twenty  best  representations  specific  \\\n",
      "0           2       1  ...       1     1                2         1   \n",
      "\n",
      "   variables  specially  vector  directional  support  three  \n",
      "0          1          1       1            1        1      1  \n",
      "\n",
      "[1 rows x 87 columns]\n",
      "         Document     consists  650000  way  neural  also  images  used  art  \\\n",
      "0  Term Frequency  1         1       1    1       2     1       1     1    1   \n",
      "\n",
      "   ...  proved  effective  deep  best  contest  neurons  training  37  \\\n",
      "0  ...       1          1     1     1        1        2         1   1   \n",
      "\n",
      "   dropout  three  \n",
      "0        1      1  \n",
      "\n",
      "[1 rows x 86 columns]\n",
      "         Document     problem  still  effort  lost  another  nodes  accepting  \\\n",
      "0  Term Frequency  1        1      1       1     1        1      2          1   \n",
      "\n",
      "   broadcast  ...  allow  attack  work  serves  best  required  basis  \\\n",
      "0          1  ...      1       1     3       1     1         1      1   \n",
      "\n",
      "   minimal  ongoing  rejoin  \n",
      "0        1        1       1  \n",
      "\n",
      "[1 rows x 84 columns]\n",
      "         Document     seasonal  reduced  virus  detection  surgical  exhaled  \\\n",
      "0  Term Frequency  1         1        2      1          2         2        1   \n",
      "\n",
      "   could  indicate  ...  illness  coronavirus  aerosols  acute  rhinoviruses  \\\n",
      "0      1         1  ...        1            2         1      1             1   \n",
      "\n",
      "   rna  droplets  face  trend  prevent  \n",
      "0    3         2     2      1        1  \n",
      "\n",
      "[1 rows x 37 columns]\n",
      "         Document     whole  distribution  believed  exploiting  hypotheses  \\\n",
      "0  Term Frequency  1      1             1         1           1           1   \n",
      "\n",
      "   distinguishable  art  tasks  ...  jiuzhang  samples  efficiency  photonic  \\\n",
      "0                1    1      1  ...         1        1           1         1   \n",
      "\n",
      "   candidate  yields  intractable  boson  indistinguishable  generates  \n",
      "0          1       1            1      2                  1          1  \n",
      "\n",
      "[1 rows x 78 columns]\n"
     ]
    }
   ],
   "source": [
    "#term -frequenvy :word occurences in a document\n",
    "#No of times each word appear in a document\n",
    "def compute_tf(docs_list):\n",
    "    for doc in docs_list:\n",
    "        doc1_lst = doc.split(\" \")\n",
    "        #create dictionaries to hold the terms(words) in file \n",
    "        #initialise the count of all words to 0 first \n",
    "        wordDict_1= dict.fromkeys(set(doc1_lst), 0) \n",
    "\n",
    "        for token in doc1_lst:\n",
    "            wordDict_1[token] +=  1\n",
    "        #Using pandas create a dataframe to visualise the data with the frequency of each word appear in each sentences    \n",
    "        df = pd.DataFrame([wordDict_1])\n",
    "        idx = 0\n",
    "        new_col = [\"Term Frequency\"]    \n",
    "        df.insert(loc=idx, column='Document', value=new_col)\n",
    "        print(df)\n",
    "        \n",
    "#return the dictionary with the TF values of the given document        \n",
    "compute_tf([doc1, doc2, doc3, doc4, doc5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b10d14b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Document     price       mse  direction   applied  tailored    future  \\\n",
      "0  Normalized TF  0.043103  0.008621   0.008621  0.008621  0.008621  0.017241   \n",
      "\n",
      "   containing    stocks  articles  ...    twenty      best  representations  \\\n",
      "0    0.017241  0.008621  0.017241  ...  0.008621  0.008621         0.017241   \n",
      "\n",
      "   specific  variables  specially    vector  directional   support     three  \n",
      "0  0.008621   0.008621   0.008621  0.008621     0.008621  0.008621  0.008621  \n",
      "\n",
      "[1 rows x 86 columns]\n",
      "        Document  consists    650000       way    neural      also    images  \\\n",
      "0  Normalized TF  0.009901  0.009901  0.009901  0.019802  0.009901  0.009901   \n",
      "\n",
      "       used       art        26  ...    proved  effective      deep      best  \\\n",
      "0  0.009901  0.009901  0.009901  ...  0.009901   0.009901  0.009901  0.009901   \n",
      "\n",
      "    contest   neurons  training        37   dropout     three  \n",
      "0  0.009901  0.019802  0.009901  0.009901  0.009901  0.009901  \n",
      "\n",
      "[1 rows x 85 columns]\n",
      "        Document   problem     still    effort      lost   another     nodes  \\\n",
      "0  Normalized TF  0.009174  0.009174  0.009174  0.009174  0.009174  0.018349   \n",
      "\n",
      "   accepting  broadcast   digital  ...     allow    attack      work  \\\n",
      "0   0.009174   0.009174  0.009174  ...  0.009174  0.009174  0.027523   \n",
      "\n",
      "     serves      best  required     basis   minimal   ongoing    rejoin  \n",
      "0  0.009174  0.009174  0.009174  0.009174  0.009174  0.009174  0.009174  \n",
      "\n",
      "[1 rows x 83 columns]\n",
      "        Document  seasonal   reduced     virus  detection  surgical   exhaled  \\\n",
      "0  Normalized TF  0.019608  0.039216  0.019608   0.039216  0.039216  0.019608   \n",
      "\n",
      "      could  indicate  identified  ...   illness  coronavirus  aerosols  \\\n",
      "0  0.019608  0.019608    0.019608  ...  0.019608     0.039216  0.019608   \n",
      "\n",
      "      acute  rhinoviruses       rna  droplets      face     trend   prevent  \n",
      "0  0.019608      0.019608  0.058824  0.039216  0.039216  0.019608  0.019608  \n",
      "\n",
      "[1 rows x 36 columns]\n",
      "        Document    whole   factor  advantage  promise  distribution  \\\n",
      "0  Normalized TF  0.01087  0.01087    0.01087  0.01087       0.01087   \n",
      "\n",
      "     photon  classical   strong  squeezed  ...  indistinguishable  generates  \\\n",
      "0  0.021739    0.01087  0.01087   0.01087  ...            0.01087    0.01087   \n",
      "\n",
      "    faster  supercomputers  uniform  sampling     state       76  detectors  \\\n",
      "0  0.01087         0.01087  0.01087  0.043478  0.021739  0.01087    0.01087   \n",
      "\n",
      "      full  \n",
      "0  0.01087  \n",
      "\n",
      "[1 rows x 77 columns]\n"
     ]
    }
   ],
   "source": [
    "#Normalized Term Frequency\n",
    "\n",
    "def termFrequency(term, document):\n",
    "    normalizeDocument = document.lower().split()\n",
    "    return normalizeDocument.count(term.lower()) / float(len(normalizeDocument))\n",
    "\n",
    "def compute_normalizedtf(documents):\n",
    "    tf_doc = []\n",
    "    for txt in documents:\n",
    "        sentence = txt.split()\n",
    "        norm_tf= dict.fromkeys(set(sentence), 0)\n",
    "        for word in sentence:\n",
    "            norm_tf[word] = termFrequency(word, txt)\n",
    "        tf_doc.append(norm_tf)\n",
    "        df = pd.DataFrame([norm_tf])\n",
    "        idx = 0\n",
    "        new_col = [\"Normalized TF\"]    \n",
    "        df.insert(loc=idx, column='Document', value=new_col)\n",
    "        print(df)\n",
    "    return tf_doc\n",
    "\n",
    "tf_doc = compute_normalizedtf([doc1, doc2, doc3, doc4, doc5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88ff3a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'research': 2.6094379124341005,\n",
       " 'examines': 2.6094379124341005,\n",
       " 'predictive': 2.6094379124341005,\n",
       " 'machine': 2.6094379124341005,\n",
       " 'learning': 2.6094379124341005,\n",
       " 'approach': 2.6094379124341005,\n",
       " 'financial': 1.916290731874155,\n",
       " 'news': 2.6094379124341005,\n",
       " 'articles': 2.6094379124341005,\n",
       " 'analysis': 2.6094379124341005,\n",
       " 'using': 1.5108256237659907,\n",
       " 'several': 2.6094379124341005,\n",
       " 'different': 1.916290731874155,\n",
       " 'textual': 2.6094379124341005,\n",
       " 'representations': 2.6094379124341005,\n",
       " 'bag': 2.6094379124341005,\n",
       " 'words': 2.6094379124341005,\n",
       " 'noun': 2.6094379124341005,\n",
       " 'phrases': 2.6094379124341005,\n",
       " 'named': 2.6094379124341005,\n",
       " 'entities': 2.6094379124341005,\n",
       " 'investigated': 2.6094379124341005,\n",
       " '9211': 2.6094379124341005,\n",
       " '10259042': 2.6094379124341005,\n",
       " 'stock': 2.6094379124341005,\n",
       " 'quotes': 2.6094379124341005,\n",
       " 'covering': 2.6094379124341005,\n",
       " '500': 2.6094379124341005,\n",
       " 'stocks': 2.6094379124341005,\n",
       " 'five': 1.916290731874155,\n",
       " 'week': 2.6094379124341005,\n",
       " 'period': 2.6094379124341005,\n",
       " 'applied': 2.6094379124341005,\n",
       " 'estimate': 2.6094379124341005,\n",
       " 'discrete': 2.6094379124341005,\n",
       " 'price': 2.6094379124341005,\n",
       " 'twenty': 2.6094379124341005,\n",
       " 'minutes': 2.6094379124341005,\n",
       " 'article': 2.6094379124341005,\n",
       " 'released': 2.6094379124341005,\n",
       " 'support': 2.6094379124341005,\n",
       " 'vector': 2.6094379124341005,\n",
       " 'svm': 2.6094379124341005,\n",
       " 'derivative': 2.6094379124341005,\n",
       " 'specially': 2.6094379124341005,\n",
       " 'tailored': 2.6094379124341005,\n",
       " 'numeric': 2.6094379124341005,\n",
       " 'prediction': 2.6094379124341005,\n",
       " 'models': 2.6094379124341005,\n",
       " 'containing': 2.6094379124341005,\n",
       " 'specific': 2.6094379124341005,\n",
       " 'variables': 2.6094379124341005,\n",
       " 'show': 2.6094379124341005,\n",
       " 'model': 1.916290731874155,\n",
       " 'terms': 2.6094379124341005,\n",
       " 'time': 2.6094379124341005,\n",
       " 'release': 2.6094379124341005,\n",
       " 'best': 1.5108256237659907,\n",
       " 'performance': 2.6094379124341005,\n",
       " 'closeness': 2.6094379124341005,\n",
       " 'actual': 2.6094379124341005,\n",
       " 'future': 2.6094379124341005,\n",
       " 'mse': 2.6094379124341005,\n",
       " '04261': 2.6094379124341005,\n",
       " 'direction': 2.6094379124341005,\n",
       " 'movement': 2.6094379124341005,\n",
       " '57': 2.6094379124341005,\n",
       " 'directional': 2.6094379124341005,\n",
       " 'accuracy': 2.6094379124341005,\n",
       " 'highest': 2.6094379124341005,\n",
       " 'return': 2.6094379124341005,\n",
       " 'simulated': 2.6094379124341005,\n",
       " 'trading': 2.6094379124341005,\n",
       " 'engine': 2.6094379124341005,\n",
       " '06': 2.6094379124341005,\n",
       " 'found': 2.6094379124341005,\n",
       " 'proper': 2.6094379124341005,\n",
       " 'scheme': 2.6094379124341005,\n",
       " 'performs': 2.6094379124341005,\n",
       " 'better': 1.916290731874155,\n",
       " 'de': 2.6094379124341005,\n",
       " 'facto': 2.6094379124341005,\n",
       " 'standard': 2.6094379124341005,\n",
       " 'three': 1.916290731874155,\n",
       " 'metrics': 2.6094379124341005,\n",
       " 'trained': 2.6094379124341005,\n",
       " 'large': 2.6094379124341005,\n",
       " 'deep': 2.6094379124341005,\n",
       " 'convolutional': 2.6094379124341005,\n",
       " 'neural': 2.6094379124341005,\n",
       " 'network': 1.916290731874155,\n",
       " 'classify': 2.6094379124341005,\n",
       " 'million': 2.6094379124341005,\n",
       " 'high': 1.916290731874155,\n",
       " 'resolution': 2.6094379124341005,\n",
       " 'images': 2.6094379124341005,\n",
       " 'imagenet': 2.6094379124341005,\n",
       " 'lsvrc': 2.6094379124341005,\n",
       " '2010': 2.6094379124341005,\n",
       " 'contest': 2.6094379124341005,\n",
       " '1000': 2.6094379124341005,\n",
       " 'classes': 2.6094379124341005,\n",
       " 'test': 2.6094379124341005,\n",
       " 'data': 2.6094379124341005,\n",
       " 'achieved': 2.6094379124341005,\n",
       " 'top': 2.6094379124341005,\n",
       " 'error': 2.6094379124341005,\n",
       " 'rates': 2.6094379124341005,\n",
       " '37': 2.6094379124341005,\n",
       " '17': 2.6094379124341005,\n",
       " 'considerably': 2.6094379124341005,\n",
       " 'previous': 2.6094379124341005,\n",
       " 'state': 1.916290731874155,\n",
       " 'art': 1.916290731874155,\n",
       " '60': 2.6094379124341005,\n",
       " 'parameters': 2.6094379124341005,\n",
       " '650000': 2.6094379124341005,\n",
       " 'neurons': 2.6094379124341005,\n",
       " 'consists': 2.6094379124341005,\n",
       " 'layers': 2.6094379124341005,\n",
       " 'followed': 2.6094379124341005,\n",
       " 'max': 2.6094379124341005,\n",
       " 'pooling': 2.6094379124341005,\n",
       " 'fully': 2.6094379124341005,\n",
       " 'connected': 2.6094379124341005,\n",
       " 'final': 2.6094379124341005,\n",
       " 'way': 2.6094379124341005,\n",
       " 'softmax': 2.6094379124341005,\n",
       " 'make': 2.6094379124341005,\n",
       " 'training': 2.6094379124341005,\n",
       " 'faster': 1.916290731874155,\n",
       " 'used': 2.6094379124341005,\n",
       " 'non': 2.6094379124341005,\n",
       " 'saturating': 2.6094379124341005,\n",
       " 'efficient': 2.6094379124341005,\n",
       " 'gpu': 2.6094379124341005,\n",
       " 'implementation': 2.6094379124341005,\n",
       " 'convolution': 2.6094379124341005,\n",
       " 'operation': 2.6094379124341005,\n",
       " 'reduce': 2.6094379124341005,\n",
       " 'overfitting': 2.6094379124341005,\n",
       " 'employed': 2.6094379124341005,\n",
       " 'recently': 2.6094379124341005,\n",
       " 'developed': 2.6094379124341005,\n",
       " 'regularization': 2.6094379124341005,\n",
       " 'method': 2.6094379124341005,\n",
       " 'called': 2.6094379124341005,\n",
       " 'dropout': 2.6094379124341005,\n",
       " 'proved': 2.6094379124341005,\n",
       " 'effective': 2.6094379124341005,\n",
       " 'also': 2.6094379124341005,\n",
       " 'entered': 2.6094379124341005,\n",
       " 'variant': 2.6094379124341005,\n",
       " 'ilsvrc': 2.6094379124341005,\n",
       " '2012': 2.6094379124341005,\n",
       " 'competition': 2.6094379124341005,\n",
       " 'winning': 2.6094379124341005,\n",
       " 'rate': 1.916290731874155,\n",
       " '15': 2.6094379124341005,\n",
       " 'compared': 2.6094379124341005,\n",
       " '26': 2.6094379124341005,\n",
       " 'second': 2.6094379124341005,\n",
       " 'entry': 2.6094379124341005,\n",
       " 'purely': 2.6094379124341005,\n",
       " 'peer': 2.6094379124341005,\n",
       " 'version': 2.6094379124341005,\n",
       " 'electronic': 2.6094379124341005,\n",
       " 'cash': 2.6094379124341005,\n",
       " 'would': 2.6094379124341005,\n",
       " 'allow': 2.6094379124341005,\n",
       " 'online': 2.6094379124341005,\n",
       " 'payments': 2.6094379124341005,\n",
       " 'sent': 2.6094379124341005,\n",
       " 'directly': 2.6094379124341005,\n",
       " 'one': 2.6094379124341005,\n",
       " 'party': 2.6094379124341005,\n",
       " 'another': 2.6094379124341005,\n",
       " 'without': 2.6094379124341005,\n",
       " 'going': 2.6094379124341005,\n",
       " 'institution': 2.6094379124341005,\n",
       " 'digital': 2.6094379124341005,\n",
       " 'signatures': 2.6094379124341005,\n",
       " 'provide': 2.6094379124341005,\n",
       " 'part': 2.6094379124341005,\n",
       " 'solution': 2.6094379124341005,\n",
       " 'main': 2.6094379124341005,\n",
       " 'benefits': 2.6094379124341005,\n",
       " 'lost': 2.6094379124341005,\n",
       " 'trusted': 2.6094379124341005,\n",
       " 'third': 2.6094379124341005,\n",
       " 'still': 2.6094379124341005,\n",
       " 'required': 2.6094379124341005,\n",
       " 'prevent': 1.916290731874155,\n",
       " 'double': 2.6094379124341005,\n",
       " 'spending': 2.6094379124341005,\n",
       " 'propose': 2.6094379124341005,\n",
       " 'problem': 2.6094379124341005,\n",
       " 'timestamps': 2.6094379124341005,\n",
       " 'transactions': 2.6094379124341005,\n",
       " 'hashing': 2.6094379124341005,\n",
       " 'ongoing': 2.6094379124341005,\n",
       " 'chain': 2.6094379124341005,\n",
       " 'hash': 2.6094379124341005,\n",
       " 'based': 2.6094379124341005,\n",
       " 'proof': 2.6094379124341005,\n",
       " 'work': 2.6094379124341005,\n",
       " 'forming': 2.6094379124341005,\n",
       " 'record': 2.6094379124341005,\n",
       " 'changed': 2.6094379124341005,\n",
       " 'redoing': 2.6094379124341005,\n",
       " 'longest': 2.6094379124341005,\n",
       " 'serves': 2.6094379124341005,\n",
       " 'sequence': 2.6094379124341005,\n",
       " 'events': 2.6094379124341005,\n",
       " 'witnessed': 2.6094379124341005,\n",
       " 'came': 2.6094379124341005,\n",
       " 'largest': 2.6094379124341005,\n",
       " 'pool': 2.6094379124341005,\n",
       " 'cpu': 2.6094379124341005,\n",
       " 'power': 2.6094379124341005,\n",
       " 'long': 2.6094379124341005,\n",
       " 'majority': 2.6094379124341005,\n",
       " 'controlled': 2.6094379124341005,\n",
       " 'nodes': 2.6094379124341005,\n",
       " 'cooperating': 2.6094379124341005,\n",
       " 'attack': 2.6094379124341005,\n",
       " 'generate': 2.6094379124341005,\n",
       " 'outpace': 2.6094379124341005,\n",
       " 'attackers': 2.6094379124341005,\n",
       " 'requires': 2.6094379124341005,\n",
       " 'minimal': 2.6094379124341005,\n",
       " 'structure': 2.6094379124341005,\n",
       " 'messages': 2.6094379124341005,\n",
       " 'broadcast': 2.6094379124341005,\n",
       " 'effort': 2.6094379124341005,\n",
       " 'basis': 2.6094379124341005,\n",
       " 'leave': 2.6094379124341005,\n",
       " 'rejoin': 2.6094379124341005,\n",
       " 'accepting': 2.6094379124341005,\n",
       " 'happened': 2.6094379124341005,\n",
       " 'gone': 2.6094379124341005,\n",
       " 'identified': 2.6094379124341005,\n",
       " 'seasonal': 2.6094379124341005,\n",
       " 'human': 2.6094379124341005,\n",
       " 'coronaviruses': 2.6094379124341005,\n",
       " 'influenza': 2.6094379124341005,\n",
       " 'viruses': 2.6094379124341005,\n",
       " 'rhinoviruses': 2.6094379124341005,\n",
       " 'exhaled': 2.6094379124341005,\n",
       " 'breath': 2.6094379124341005,\n",
       " 'coughs': 2.6094379124341005,\n",
       " 'children': 2.6094379124341005,\n",
       " 'adults': 2.6094379124341005,\n",
       " 'acute': 2.6094379124341005,\n",
       " 'respiratory': 2.6094379124341005,\n",
       " 'illness': 2.6094379124341005,\n",
       " 'surgical': 2.6094379124341005,\n",
       " 'face': 2.6094379124341005,\n",
       " 'masks': 2.6094379124341005,\n",
       " 'significantly': 2.6094379124341005,\n",
       " 'reduced': 2.6094379124341005,\n",
       " 'detection': 2.6094379124341005,\n",
       " 'virus': 2.6094379124341005,\n",
       " 'rna': 2.6094379124341005,\n",
       " 'droplets': 2.6094379124341005,\n",
       " 'coronavirus': 2.6094379124341005,\n",
       " 'aerosols': 2.6094379124341005,\n",
       " 'trend': 2.6094379124341005,\n",
       " 'toward': 2.6094379124341005,\n",
       " 'results': 2.6094379124341005,\n",
       " 'indicate': 2.6094379124341005,\n",
       " 'could': 2.6094379124341005,\n",
       " 'transmission': 2.6094379124341005,\n",
       " 'symptomatic': 2.6094379124341005,\n",
       " 'individuals': 2.6094379124341005,\n",
       " 'quantum': 2.6094379124341005,\n",
       " 'computers': 2.6094379124341005,\n",
       " 'promise': 2.6094379124341005,\n",
       " 'perform': 2.6094379124341005,\n",
       " 'certain': 2.6094379124341005,\n",
       " 'tasks': 2.6094379124341005,\n",
       " 'believed': 2.6094379124341005,\n",
       " 'intractable': 2.6094379124341005,\n",
       " 'classical': 2.6094379124341005,\n",
       " 'boson': 2.6094379124341005,\n",
       " 'sampling': 2.6094379124341005,\n",
       " 'task': 2.6094379124341005,\n",
       " 'considered': 2.6094379124341005,\n",
       " 'strong': 2.6094379124341005,\n",
       " 'candidate': 2.6094379124341005,\n",
       " 'demonstrate': 2.6094379124341005,\n",
       " 'computational': 2.6094379124341005,\n",
       " 'advantage': 2.6094379124341005,\n",
       " 'performed': 2.6094379124341005,\n",
       " 'gaussian': 2.6094379124341005,\n",
       " 'sending': 2.6094379124341005,\n",
       " '50': 2.6094379124341005,\n",
       " 'indistinguishable': 2.6094379124341005,\n",
       " 'single': 2.6094379124341005,\n",
       " 'mode': 2.6094379124341005,\n",
       " 'squeezed': 2.6094379124341005,\n",
       " 'states': 2.6094379124341005,\n",
       " '100': 2.6094379124341005,\n",
       " 'ultralow': 2.6094379124341005,\n",
       " 'loss': 2.6094379124341005,\n",
       " 'interferometer': 2.6094379124341005,\n",
       " 'full': 2.6094379124341005,\n",
       " 'connectivity': 2.6094379124341005,\n",
       " 'random': 2.6094379124341005,\n",
       " 'matrix—the': 2.6094379124341005,\n",
       " 'whole': 2.6094379124341005,\n",
       " 'optical': 2.6094379124341005,\n",
       " 'setup': 2.6094379124341005,\n",
       " 'phase': 2.6094379124341005,\n",
       " 'locked—and': 2.6094379124341005,\n",
       " 'output': 2.6094379124341005,\n",
       " 'efficiency': 2.6094379124341005,\n",
       " 'photon': 2.6094379124341005,\n",
       " 'detectors': 2.6094379124341005,\n",
       " 'obtained': 2.6094379124341005,\n",
       " 'samples': 2.6094379124341005,\n",
       " 'validated': 2.6094379124341005,\n",
       " 'plausible': 2.6094379124341005,\n",
       " 'hypotheses': 2.6094379124341005,\n",
       " 'exploiting': 2.6094379124341005,\n",
       " 'thermal': 2.6094379124341005,\n",
       " 'distinguishable': 2.6094379124341005,\n",
       " 'photons': 2.6094379124341005,\n",
       " 'uniform': 2.6094379124341005,\n",
       " 'distribution': 2.6094379124341005,\n",
       " 'photonic': 2.6094379124341005,\n",
       " 'computer': 2.6094379124341005,\n",
       " 'jiuzhang': 2.6094379124341005,\n",
       " 'generates': 2.6094379124341005,\n",
       " '76': 2.6094379124341005,\n",
       " 'clicks': 2.6094379124341005,\n",
       " 'yields': 2.6094379124341005,\n",
       " 'space': 2.6094379124341005,\n",
       " 'dimension': 2.6094379124341005,\n",
       " '1030': 2.6094379124341005,\n",
       " 'simulation': 2.6094379124341005,\n",
       " 'strategy': 2.6094379124341005,\n",
       " 'supercomputers': 2.6094379124341005,\n",
       " 'factor': 2.6094379124341005,\n",
       " '1014': 2.6094379124341005}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IDF(term) = 1 + log(Total Number Of Documents / Number Of Documents with term 'term' in it)\n",
    "#to weigh up the effects of less frequently occurring terms.\n",
    "\n",
    "def inverseDocumentFrequency(term, allDocuments):\n",
    "    numDocumentsWithThisTerm = 0 #calculate the values of Inverse Document Frequency\n",
    "    for doc in range (0, len(allDocuments)): #Take the number of documents using len function\n",
    "        if term.lower() in allDocuments[doc].lower().split():\n",
    "            numDocumentsWithThisTerm = numDocumentsWithThisTerm + 1 #count the documents contain specific word in it\n",
    " \n",
    "    if numDocumentsWithThisTerm > 0:\n",
    "        return 1.0 + math.log(float(len(allDocuments)) / numDocumentsWithThisTerm)\n",
    "    else:\n",
    "        return 1.0\n",
    "    \n",
    "def compute_idf(documents):\n",
    "    idf_dict = {} #Create a dictionary to hold the IDF values.\n",
    "    for doc in documents:\n",
    "        sentence = doc.split()\n",
    "        for word in sentence: #traverse through all the documents in the doclist\n",
    "            idf_dict[word] = inverseDocumentFrequency(word, documents)\n",
    "    return idf_dict\n",
    "idf_dict = compute_idf([doc1, doc2, doc3, doc4, doc5])\n",
    "\n",
    "compute_idf([doc1, doc2, doc3, doc4, doc5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20cfd547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   doc   trained      deep  convolutional    neural   network        60  \\\n",
      "0    0  0.000000  0.000000       0.000000  0.000000  0.000000  0.000000   \n",
      "1    1  0.025836  0.025836       0.051672  0.051672  0.037946  0.025836   \n",
      "2    2  0.000000  0.000000       0.000000  0.000000  0.087903  0.000000   \n",
      "3    3  0.000000  0.000000       0.000000  0.000000  0.000000  0.000000   \n",
      "4    4  0.000000  0.000000       0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "    million  parameters    650000  ...  classify      high  resolution  \\\n",
      "0  0.000000    0.000000  0.000000  ...  0.000000  0.000000    0.000000   \n",
      "1  0.051672    0.025836  0.025836  ...  0.025836  0.018973    0.025836   \n",
      "2  0.000000    0.000000  0.000000  ...  0.000000  0.000000    0.000000   \n",
      "3  0.000000    0.000000  0.000000  ...  0.000000  0.000000    0.000000   \n",
      "4  0.000000    0.000000  0.000000  ...  0.000000  0.020829    0.000000   \n",
      "\n",
      "     images  imagenet     lsvrc      2010   contest  different   classes  \n",
      "0  0.000000  0.000000  0.000000  0.000000  0.000000   0.049559  0.000000  \n",
      "1  0.025836  0.025836  0.025836  0.025836  0.025836   0.018973  0.025836  \n",
      "2  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000  0.000000  \n",
      "3  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000  0.000000  \n",
      "4  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000  0.000000  \n",
      "\n",
      "[5 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "# tf-idf score across all docs for the query string\n",
    "#This function will calculate the TF*IDF value in a dictionary format\n",
    "def compute_tfidf_with_alldocs(documents , query):\n",
    "    tf_idf = []\n",
    "    index = 0\n",
    "    query_tokens = query.split()\n",
    "    df = pd.DataFrame(columns=['doc'] + query_tokens)\n",
    "    for doc in documents:\n",
    "        df['doc'] = np.arange(0 , len(documents))\n",
    "        doc_num = tf_doc[index]\n",
    "        sentence = doc.split()\n",
    "        for word in sentence:\n",
    "            for text in query_tokens:\n",
    "                if(text == word):\n",
    "                    idx = sentence.index(word)\n",
    "                    tf_idf_score = doc_num[word] * idf_dict[word]\n",
    "                    tf_idf.append(tf_idf_score)\n",
    "                    df.iloc[index, df.columns.get_loc(word)] = tf_idf_score\n",
    "        index += 1\n",
    "    df.fillna(0 , axis=1, inplace=True)\n",
    "    return tf_idf , df\n",
    "            \n",
    "documents = [doc1, doc2, doc3, doc4, doc5]\n",
    "tf_idf , df = compute_tfidf_with_alldocs(documents , query)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31a6d5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'trained': 0.030303030303030304, 'deep': 0.030303030303030304, 'convolutional': 0.030303030303030304, 'neural': 0.030303030303030304, 'network': 0.030303030303030304, '60': 0.030303030303030304, 'million': 0.030303030303030304, 'parameters': 0.030303030303030304, '650000': 0.030303030303030304, 'neurons': 0.030303030303030304, 'consists': 0.030303030303030304, 'five': 0.030303030303030304, 'layers': 0.030303030303030304, 'followed': 0.030303030303030304, 'max': 0.030303030303030304, 'pooling': 0.030303030303030304, 'three': 0.030303030303030304, 'fully': 0.030303030303030304, 'connected': 0.030303030303030304, 'final': 0.030303030303030304, '1000': 0.030303030303030304, 'way': 0.030303030303030304, 'softmax': 0.030303030303030304, 'classify': 0.030303030303030304, 'high': 0.030303030303030304, 'resolution': 0.030303030303030304, 'images': 0.030303030303030304, 'imagenet': 0.030303030303030304, 'lsvrc': 0.030303030303030304, '2010': 0.030303030303030304, 'contest': 0.030303030303030304, 'different': 0.030303030303030304, 'classes': 0.030303030303030304}\n"
     ]
    }
   ],
   "source": [
    "#TF for the query string\n",
    "def compute_query_tf(query):\n",
    "    query_norm_tf = {}\n",
    "    tokens = query.split()\n",
    "    for word in tokens:\n",
    "        query_norm_tf[word] = termFrequency(word , query)\n",
    "    return query_norm_tf\n",
    "query_norm_tf = compute_query_tf(query)\n",
    "print(query_norm_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91d65156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'trained': 2.6094379124341005, 'deep': 2.6094379124341005, 'convolutional': 2.6094379124341005, 'neural': 2.6094379124341005, 'network': 1.916290731874155, '60': 2.6094379124341005, 'million': 2.6094379124341005, 'parameters': 2.6094379124341005, '650000': 2.6094379124341005, 'neurons': 2.6094379124341005, 'consists': 2.6094379124341005, 'five': 1.916290731874155, 'layers': 2.6094379124341005, 'followed': 2.6094379124341005, 'max': 2.6094379124341005, 'pooling': 2.6094379124341005, 'three': 1.916290731874155, 'fully': 2.6094379124341005, 'connected': 2.6094379124341005, 'final': 2.6094379124341005, '1000': 2.6094379124341005, 'way': 2.6094379124341005, 'softmax': 2.6094379124341005, 'classify': 2.6094379124341005, 'high': 1.916290731874155, 'resolution': 2.6094379124341005, 'images': 2.6094379124341005, 'imagenet': 2.6094379124341005, 'lsvrc': 2.6094379124341005, '2010': 2.6094379124341005, 'contest': 2.6094379124341005, 'different': 1.916290731874155, 'classes': 2.6094379124341005}\n"
     ]
    }
   ],
   "source": [
    "#idf score for the query string\n",
    "def compute_query_idf(query):\n",
    "    idf_dict_qry = {}\n",
    "    sentence = query.split()\n",
    "    documents = [doc1, doc2, doc3, doc4, doc5]\n",
    "    for word in sentence:\n",
    "        idf_dict_qry[word] = inverseDocumentFrequency(word ,documents)\n",
    "    return idf_dict_qry\n",
    "idf_dict_qry = compute_query_idf(query)\n",
    "print(idf_dict_qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7eef2f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'trained': 0.07907387613436669, 'deep': 0.07907387613436669, 'convolutional': 0.07907387613436669, 'neural': 0.07907387613436669, 'network': 0.05806941611739864, '60': 0.07907387613436669, 'million': 0.07907387613436669, 'parameters': 0.07907387613436669, '650000': 0.07907387613436669, 'neurons': 0.07907387613436669, 'consists': 0.07907387613436669, 'five': 0.05806941611739864, 'layers': 0.07907387613436669, 'followed': 0.07907387613436669, 'max': 0.07907387613436669, 'pooling': 0.07907387613436669, 'three': 0.05806941611739864, 'fully': 0.07907387613436669, 'connected': 0.07907387613436669, 'final': 0.07907387613436669, '1000': 0.07907387613436669, 'way': 0.07907387613436669, 'softmax': 0.07907387613436669, 'classify': 0.07907387613436669, 'high': 0.05806941611739864, 'resolution': 0.07907387613436669, 'images': 0.07907387613436669, 'imagenet': 0.07907387613436669, 'lsvrc': 0.07907387613436669, '2010': 0.07907387613436669, 'contest': 0.07907387613436669, 'different': 0.05806941611739864, 'classes': 0.07907387613436669}\n"
     ]
    }
   ],
   "source": [
    "#tf-idf score for the query string\n",
    "def compute_query_tfidf(query):\n",
    "    tfidf_dict_qry = {}\n",
    "    sentence = query.split()\n",
    "    for word in sentence:\n",
    "        tfidf_dict_qry[word] = query_norm_tf[word] * idf_dict_qry[word]\n",
    "    return tfidf_dict_qry\n",
    "tfidf_dict_qry = compute_query_tfidf(query)\n",
    "print(tfidf_dict_qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a08ce68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine Similarity(Query,Document1) = Dot product(Query, Document1) / ||Query|| * ||Document1||\n",
    "def cosine_similarity(tfidf_dict_qry, df , query , doc_num):\n",
    "    dot_product = 0\n",
    "    qry_mod = 0\n",
    "    doc_mod = 0\n",
    "    tokens = query.split()\n",
    "   \n",
    "    for keyword in tokens:\n",
    "        dot_product += tfidf_dict_qry[keyword] * df[keyword][df['doc'] == doc_num]\n",
    "        #||Query||\n",
    "        qry_mod += tfidf_dict_qry[keyword] * tfidf_dict_qry[keyword]\n",
    "        #||Document||\n",
    "        doc_mod += df[keyword][df['doc'] == doc_num] * df[keyword][df['doc'] == doc_num]\n",
    "    qry_mod = np.sqrt(qry_mod)\n",
    "    doc_mod = np.sqrt(doc_mod)\n",
    "    #implement formula\n",
    "    denominator = qry_mod * doc_mod\n",
    "    cos_sim = dot_product/denominator\n",
    "     \n",
    "    return cos_sim\n",
    "\n",
    "def flatten(lis):\n",
    "     for item in lis:\n",
    "        if isinstance(item, Iterable) and not isinstance(item, str):\n",
    "             for x in flatten(item):\n",
    "                yield x\n",
    "        else:        \n",
    "             yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99c10124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Document1', 'Document2', 'Document3', 'Document4', 'Document5']\n",
      "[0.1998221977461124, 0.900479562984548, 0.13254705094161104, nan, 0.13254705094161107]\n"
     ]
    }
   ],
   "source": [
    "def rank_similarity_docs(data):    \n",
    "    cos_sim =[]\n",
    "    for doc_num in range(0 , len(data)):\n",
    "        cos_sim.append(cosine_similarity(tfidf_dict_qry, df , query , doc_num).tolist())\n",
    "    return cos_sim\n",
    "\n",
    "similarity_docs = rank_similarity_docs(documents)\n",
    "doc_names = [\"Document1\", \"Document2\", \"Document3\", \"Document4\", \"Document5\"]\n",
    "print(doc_names)\n",
    "doc_values = list(flatten(similarity_docs))\n",
    "print(doc_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "471ff07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.900479562984548\n"
     ]
    }
   ],
   "source": [
    "print(max(doc_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c11c261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {doc_names[i]: doc_values[i] for i in range(len(doc_names))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5f6bbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Document1': 0.1998221977461124, 'Document2': 0.900479562984548, 'Document3': 0.13254705094161104, 'Document4': nan, 'Document5': 0.13254705094161107}\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22cb9a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Document2', 0.900479562984548)\n"
     ]
    }
   ],
   "source": [
    "print(max(res.items(),key = lambda i : i[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888b79b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c79523",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
